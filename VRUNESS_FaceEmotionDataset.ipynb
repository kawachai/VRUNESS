{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4glvkbbP11aGhTTj5ZpVr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kawachai/VRUNESS/blob/main/VRUNESS_FaceEmotionDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vrR5TAAPCw7"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå\n",
        "# VRUNESS face emotion by Chai Meenorngwar\n",
        "# ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏†‡∏≤‡∏û\n",
        "emotions = [\"angry\", \"disgusted\", \"fearful\", \"happy\", \"neutral\", \"sad\", \"surprised\"]\n",
        "source_dir = \"raw_images\"\n",
        "\n",
        "# Create the raw_images directory if it doesn't exist\n",
        "if not os.path.exists(source_dir):\n",
        "    os.makedirs(source_dir)\n",
        "\n",
        "# Create subdirectories for each emotion\n",
        "for emotion in emotions:\n",
        "    emotion_source_dir = os.path.join(source_dir, emotion)\n",
        "    if not os.path.exists(emotion_source_dir):\n",
        "        os.makedirs(emotion_source_dir)\n",
        "\n",
        "print(\"Created raw_images directory and emotion subdirectories.\")\n",
        "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÅ‡∏•‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏†‡∏≤‡∏û\n",
        "source_dir = \"raw_images\"  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û\n",
        "dataset_dir = \"dataset\"    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Å‡πá‡∏ö dataset\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö dataset ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ\n",
        "for emotion in emotions:\n",
        "    emotion_dir = os.path.join(dataset_dir, emotion)\n",
        "    if not os.path.exists(emotion_dir):\n",
        "        os.makedirs(emotion_dir)\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î Haar Cascade\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå\n",
        "for emotion in emotions:\n",
        "    emotion_source_dir = os.path.join(source_dir, emotion)\n",
        "    images = os.listdir(emotion_source_dir)\n",
        "    count = 0\n",
        "\n",
        "    for img_name in images:\n",
        "        img_path = os.path.join(emotion_source_dir, img_name)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "        for (x, y, w, h) in faces:\n",
        "            face_img = gray[y:y+h, x:x+w]\n",
        "            face_img = cv2.resize(face_img, (48,48))\n",
        "            save_path = os.path.join(dataset_dir, emotion, f\"{count}.jpg\")\n",
        "            cv2.imwrite(save_path, face_img)\n",
        "            count += 1\n",
        "\n",
        "    print(f\"‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå {emotion}, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏†‡∏≤‡∏û {count} ‡∏£‡∏π‡∏õ\")\n",
        "\n",
        "print(\"‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå ‡πÉ‡∏ô‡∏°‡∏∏‡∏°‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
        "# VRUNESS face emotion by Chai Meenorngwar\n",
        "# ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏†‡∏≤‡∏û\n",
        "emotions = [\"angry\", \"disgusted\", \"fearful\", \"happy\", \"neutral\", \"sad\", \"surprised\"]\n",
        "source_dir = \"raw_images\"\n",
        "\n",
        "# Create the raw_images directory if it doesn't exist\n",
        "if not os.path.exists(source_dir):\n",
        "    os.makedirs(source_dir)\n",
        "\n",
        "# Create subdirectories for each emotion\n",
        "for emotion in emotions:\n",
        "    emotion_source_dir = os.path.join(source_dir, emotion)\n",
        "    if not os.path.exists(emotion_source_dir):\n",
        "        os.makedirs(emotion_source_dir)\n",
        "\n",
        "print(\"Created raw_images directory and emotion subdirectories.\")\n",
        "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÅ‡∏•‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏†‡∏≤‡∏û\n",
        "source_dir = \"raw_images\"  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û\n",
        "dataset_dir = \"dataset\"    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Å‡πá‡∏ö dataset\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö dataset ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ\n",
        "for emotion in emotions:\n",
        "    emotion_dir = os.path.join(dataset_dir, emotion)\n",
        "    if not os.path.exists(emotion_dir):\n",
        "        os.makedirs(emotion_dir)\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î Haar Cascade\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå\n",
        "for emotion in emotions:\n",
        "    emotion_source_dir = os.path.join(source_dir, emotion)\n",
        "    images = os.listdir(emotion_source_dir)\n",
        "    count = 0\n",
        "\n",
        "    for img_name in images:\n",
        "        img_path = os.path.join(emotion_source_dir, img_name)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "        for (x, y, w, h) in faces:\n",
        "            face_img = gray[y:y+h, x:x+w]\n",
        "            face_img = cv2.resize(face_img, (48,48))\n",
        "            save_path = os.path.join(dataset_dir, emotion, f\"{count}.jpg\")\n",
        "            cv2.imwrite(save_path, face_img)\n",
        "            count += 1\n",
        "\n",
        "    print(f\"‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå {emotion}, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏†‡∏≤‡∏û {count} ‡∏£‡∏π‡∏õ\")\n",
        "\n",
        "print(\"‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\")\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# ----------------------------\n",
        "# 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
        "# ----------------------------\n",
        "input_dir = \"raw_images\"\n",
        "output_dir = \"augmented_dataset\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ----------------------------\n",
        "# 2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Data Augmentation\n",
        "# ----------------------------\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,          # ‡∏´‡∏°‡∏∏‡∏ô ¬±15¬∞\n",
        "    width_shift_range=0.1,      # ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏ã‡πâ‡∏≤‡∏¢‡∏Ç‡∏ß‡∏≤ 10%\n",
        "    height_shift_range=0.1,     # ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏•‡∏á 10%\n",
        "    brightness_range=[0.7, 1.3],# ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡πà‡∏≤‡∏á\n",
        "    shear_range=0.1,            # ‡∏ö‡∏¥‡∏î‡∏†‡∏≤‡∏û\n",
        "    zoom_range=0.1,             # ‡∏ã‡∏π‡∏° ¬±10%\n",
        "    horizontal_flip=True,       # ‡∏û‡∏•‡∏¥‡∏Å‡∏ã‡πâ‡∏≤‡∏¢‡∏Ç‡∏ß‡∏≤\n",
        "    fill_mode='nearest'         # ‡πÄ‡∏ï‡∏¥‡∏°‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# 3. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Data Augmentation\n",
        "# ----------------------------\n",
        "for emotion in os.listdir(input_dir):\n",
        "    emotion_path = os.path.join(input_dir, emotion)\n",
        "    if not os.path.isdir(emotion_path):\n",
        "        continue\n",
        "\n",
        "    save_path = os.path.join(output_dir, emotion)\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    for img_name in os.listdir(emotion_path):\n",
        "        img_path = os.path.join(emotion_path, img_name)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏ô‡∏≤‡∏î‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô ‡πÄ‡∏ä‡πà‡∏ô 48x48 (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå)\n",
        "        img = cv2.resize(img, (48, 48))\n",
        "        img_array = np.expand_dims(img, 0)  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏¥‡∏ï‡∏¥ batch\n",
        "\n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Augmented Images 5 ‡∏£‡∏π‡∏õ‡∏ï‡πà‡∏≠ 1 ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö\n",
        "        aug_iter = datagen.flow(img_array, batch_size=1, save_to_dir=save_path,\n",
        "                                save_prefix=emotion, save_format='jpg')\n",
        "        for _ in range(5):\n",
        "            next(aug_iter)\n",
        "\n",
        "print(\"‚úÖ Data Augmentation ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‚Äî Dataset ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN ‡πÅ‡∏•‡πâ‡∏ß!\")"
      ],
      "metadata": {
        "id": "a4Zu96P6Qk04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Model ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô dataset\n",
        "# VRUNESS face emotion by Chai Meenorngwar\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# 1. ‡πÇ‡∏´‡∏•‡∏î Dataset\n",
        "# ----------------------------\n",
        "data_dir = \"augmented_dataset\"\n",
        "img_size = 48\n",
        "X = []\n",
        "y = []\n",
        "label_map = {}\n",
        "\n",
        "for idx, emotion in enumerate(os.listdir(data_dir)):\n",
        "    label_map[idx] = emotion\n",
        "    emotion_path = os.path.join(data_dir, emotion)\n",
        "    for img_name in os.listdir(emotion_path):\n",
        "        img_path = os.path.join(emotion_path, img_name)\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô grayscale\n",
        "        if img is None:\n",
        "            continue\n",
        "        img = cv2.resize(img, (img_size, img_size))\n",
        "        X.append(img)\n",
        "        y.append(idx)\n",
        "\n",
        "X = np.array(X).reshape(-1, img_size, img_size, 1) / 255.0  # Normalize\n",
        "y = to_categorical(y, num_classes=len(label_map))\n",
        "\n",
        "print(f\"üì¶ Dataset Loaded: {X.shape[0]} images, {len(label_map)} classes\")\n",
        "\n",
        "# ----------------------------\n",
        "# 2. ‡πÅ‡∏ö‡πà‡∏á Train/Test\n",
        "# ----------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ----------------------------\n",
        "# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN\n",
        "# ----------------------------\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(label_map), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# ----------------------------\n",
        "# 4. ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "# ----------------------------\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "                    epochs=30, batch_size=32)\n",
        "\n",
        "# ----------------------------\n",
        "# 5. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "# ----------------------------\n",
        "model.save(\"emotion_cnn_model.h5\")\n",
        "print(\"‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô emotion_cnn_model.h5\")\n",
        "\n",
        "# ----------------------------\n",
        "# 6. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•\n",
        "# ----------------------------\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"üéØ Test Accuracy: {acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "yL7H3ZAuRHGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# VRUNESS face emotion by Chai Meenorngwar\n",
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥\n",
        "# ===============================\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡πÑ‡∏ß‡πâ\n",
        "model = load_model(\"emotion_cnn_model.h5\")\n",
        "\n",
        "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î label ‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î Haarcascade ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
        "img = cv2.imread(\"image1.jpg\")\n",
        "\n",
        "if img is None:\n",
        "    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå image1.jpg\")\n",
        "else:\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        roi_gray = gray[y:y+h, x:x+w]\n",
        "        roi_gray = cv2.resize(roi_gray, (48, 48))\n",
        "        roi_gray = roi_gray / 255.0\n",
        "        roi_gray = np.reshape(roi_gray, (1, 48, 48, 1))\n",
        "\n",
        "        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•\n",
        "        prediction = model.predict(roi_gray)\n",
        "        label = emotion_labels[np.argmax(prediction)]\n",
        "        confidence = np.max(prediction)\n",
        "\n",
        "        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ö‡∏ô‡∏†‡∏≤‡∏û\n",
        "        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        cv2.putText(img, f\"{label} ({confidence:.2f})\", (x, y-10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "        print(f\"‚úÖ ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {label} (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à {confidence:.2f})\")\n",
        "\n",
        "    # ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "90ZiX7ZxR_JR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}